## Fig 2.4 
![Fig2.4](https://github.com/livey/reinforcement-learning/blob/master/Figures/Fig2_4.png)

## Exercise 4.4 
### First policy
![pi1](https://github.com/livey/reinforcement-learning/blob/master/Figures/pi1.png)
### second policy
![pi2](https://github.com/livey/reinforcement-learning/blob/master/Figures/pi2.png)
### optimal policy
![pi3](https://github.com/livey/reinforcement-learning/blob/master/Figures/optpi.png)
### final state values
![stateValues](https://github.com/livey/reinforcement-learning/blob/master/Figures/finalStateValue.png)

## Fig 5.1
### state values, @ state no useble ace, 10k iteratioins
![nousableace1](https://github.com/livey/reinforcement-learning/blob/master/Figures/noace10k.png)
### state values, @ state no usable ace, 50k iteratioins
![nousableace5](https://github.com/livey/reinforcement-learning/blob/master/Figures/noace50k.png)
### state values, @ state useble ace, 10k iteratioins
![usableace1](https://github.com/livey/reinforcement-learning/blob/master/Figures/useace10k.png)
### state values, @ state no useble ace, 50k iteratioins
![usableace5](https://github.com/livey/reinforcement-learning/blob/master/Figures/useace50k.png)

## Fig 5.4
![importanceSampling](https://github.com/livey/reinforcement-learning/blob/master/Figures/importanceSampling.png)

## Fig 6.5
### after one run 
![cliff1](https://github.com/livey/reinforcement-learning/blob/master/Figures/cliff_1run.png)
### after 40 runs
![cliff2](https://github.com/livey/reinforcement-learning/blob/master/Figures/cliff_40runs.png)

## Fig 7.2
### random walk 100 runs
![randomwalk1](https://github.com/livey/reinforcement-learning/blob/master/Figures/100repeatsrandomwalk.png)
### random walk 1000 runs
![randomwalk2](https://github.com/livey/reinforcement-learning/blob/master/Figures/1000repeatsrandomwalk.png)

## Fig 8.5
![fig8.5](https://github.com/livey/reinforcement-learning/blob/master/Figures/fig8_5.png)

## Fig 9.10
![randomwalk](https://github.com/livey/reinforcement-learning/blob/master/Figures/fig9_10.png)
