## Fig2.4 
![Fig2.4](https://github.com/livey/reinforcement-learning/blob/master/Figures/Fig2_4.png)

## Exercise 4.4 
### First policy
![pi1](https://github.com/livey/reinforcement-learning/blob/master/Figures/pi1.png)
### second policy
![pi2](https://github.com/livey/reinforcement-learning/blob/master/Figures/pi2.png)
### optimal policy
![pi3](https://github.com/livey/reinforcement-learning/blob/master/Figures/optpi.png)
### final state values
![stateValues](https://github.com/livey/reinforcement-learning/blob/master/Figures/finalStateValue.png)

## Fig5.1
### state values, @ state no useble ace, 10k iteratioins
![nousableace1](https://github.com/livey/reinforcement-learning/blob/master/Figures/noace10k.png)
### state values, @ state no usable ace, 50k iteratioins
![nousableace5](https://github.com/livey/reinforcement-learning/blob/master/Figures/noace50k.png)
### state values, @ state useble ace, 10k iteratioins
![usableace1](https://github.com/livey/reinforcement-learning/blob/master/Figures/useace10k.png)
### state values, @ state no useble ace, 50k iteratioins
![usableace5](https://github.com/livey/reinforcement-learning/blob/master/Figures/useace50k.png)
